# Lab 4B; Data sources {#lab4bopendata}

## Aims

This lesson is about getting data. We already saw in Chapter \@ref(lab4aimportingdata) that external data (contained in data files) can be read into R in a number of different ways. Here we focus on data that is not generated by yourself or available as a local data file. This chapter focusses on getting your hands on Open Data or externally available data. For a shortlist op popular Open Data sources see:

[Open Data](https://www.freecodecamp.org/news/https-medium-freecodecamp-org-best-free-open-data-sources-anyone-can-use-a65b514b0f2d/)

To get Open Data data into R you can use a number of methods.
Depending on the aim or nature of your problem or challenge you can either use: 
 
  1. Data in R packages
  1. Download files from a website or other internet source
  1. Repositories: Data-repos / Github
  1. Scientific Databases
  1. Data Science Contests (Kaggle)
  1. Amazon WebServices - S3 buckets
  1. Dedicated API's
  1. Through scraping of websites
  
## Open Science

Open Data is an an important prerequisite and concept contained within the Open Science framework. Tih sframework shortly entails the following concepts: 

 - Information (meta data), data and analysis (code and tools) are combined and stored _together_
 - Freely available (under e.g. CC-BY-NC 4.0 LICENCE)
 - Share scientific information before and during study, not only when finished
 - Open during the whole process and in every step of doing science
 - Reproducibility is key
 - Share data, methods and code on open platforms (e.g. Github.com)

In this chapter we will go over an example for each of these methods.

## Data in R packages
  
### **EXERCISE 1 - base R `{datasets}`** {-} 

The R base installation comes with a package called `{datasets}`. This package is loaded when you start RStudio. 

A) Type `?datasets` in the Console.

B) List all datasets from the `{datasets}` package
```{r, echo=FALSE, results='hide'}
data(package = "datasets")
```

C) Load the `Titanic` dataset from the `{datasets}` package
```{r, include=FALSE, eval=FALSE}
data(package = "datasets", "Titanic")
titanic <- Titanic %>% as_tibble
head(titanic, 3)
```

### **EXERCISE 2 - Access data included in R packages** {-}

Often, data is included in R packages for examples on how to use methods proposed in the package or to illustrate statistical principles.

Load the dataset "BostonHousing" from the `{mlbench}` package
```{r, include=FALSE, eval=FALSE}
library(mlbench)
## to load Boston Housing Dataset
data("BostonHousing", package = "mlbench")
```

### Data only packages {-}
Sometimes all a package is, is data. For example the full human genome sequence can be downloaded as a bioconductor package. Or some machine learning datasets from other packages are available as a datapackage.
Here we load the `wine` dataset from the `{rattle.data}` package.

```{r, eval=FALSE}
data(package = "rattle.data")
help(wine, package = "rattle.data")
```

Or the full standardized human genome with: 
We show 101 nucleotides from chromosome 1, location 30,000 - 30,100
```{r}
# BiocManager::install("BSgenome.Hsapiens.UCSC.hg38")
library(BSgenome.Hsapiens.UCSC.hg38)
BSgenome.Hsapiens.UCSC.hg38$chr1[30000:30100]
```

### The `wine` dataset
The `wine` dataset is an example dataset, used for teaching and discovering machine learning. We will come back to this dataset in Chapter \@ref(lab6assumptionsmodels)

### **EXERCISE 3** {-}

Load the wine dataset from the `{ratlle.data}` package
```{r}
data(package = "rattle.data", "wine")
```

### **EXERCISE 4: Exploring the `wine` data with PCA** {-}

When exploring data like the wine dataset that has numerous numeric features to decribe or classify an observation, we can perform a Principal Component Analysis (PCA).

A) Review a demo on PCA here: http://huboqiang.cn/2016/03/03/RscatterPlotPCA
Using the information from this demo, perform and plot a PCA analysis (plot only PC1 vs PC2 en PC1 vs PC3). Remember that for principal component analysis you best transfrom the data by scaling and centering.

when preparing data for PCA data all individual data points are centered and scaled. 

Run a PCA analysis on the 'wine' dataset. Use the information from the demo. 
```{r, include=FALSE}
names(wine)
pca_data <- wine[,c(-1)] %>% 
  log10(.) %>%
  stats::prcomp(.,
                center = TRUE,
                scale = TRUE) 

pca_data$x %>%  
  as_tibble() %>%
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point() +
  theme_bw() 
```

B) Add colour to the points for the `Type` variable {-}
Write a piece of code that generates the following graph
```{r, echo=FALSE, results='hide'}
labels <- wine[,1]
pca_data$x %>%  
  as_tibble() %>%
  mutate(Type = labels) %>%
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point(aes(colour = Type)) +
  theme_bw() 
```

--- EXERCISE END ---

## Download files from a website or other internet source

Many files are downloadable as tab-seperated or comma-separated values. Often these files are hosted on a website of open-data repository. They can be downloaded directly from the url.

Visit the following page:
[Meldingen openbare ruimte Utrecht](https://ckan.dataplatform.nl/dataset/mor-utrecht/resource/e16e9af3-8dbc-4f82-aecf-1c69b628c81d)

### Download from an url with `download.file()`
The code below downloads the `.csv` of the above webpage file to the `/data` dir.

```{r}
url <- "https://ckan.dataplatform.nl/dataset/7dc70520-1653-49f6-a251-c95939bb6962/resource/e16e9af3-8dbc-4f82-aecf-1c69b628c81d/download/meldingen2015open-data.csv"

download.file(url = url, destfile = here::here(
  "data",
  "meldingen2015open-data.csv"
))
``` 

### Loading this dataset from Utrecht, The Netherlands into R

```{r}
meldingen2015open_data <- read_csv(here::here(
  "data",
  "meldingen2015open-data.csv"
))
```

### How many and which notifications (main categories) per Utrecht area?

```{r}
names(meldingen2015open_data)

meldingen2015open_data %>%
  group_by(Hoofdcategorie, Wijk) %>%
  tally() %>%
  ggplot(aes(x = reorder(as_factor(Hoofdcategorie), n), y = n)) +
  geom_point(aes(colour = Wijk)) +
    coord_flip()
```

## Repositories: Data-repos / Github

There are many (too many to mention all here) dedicated datasources in the form of data repositories. I will show three examples here: Google datasets and Github.com as valuable sources for data.

## Via Google Datasets

I entered the search terms "Canis lupus" AND "Europe"
One of the hits was this link:

[download link](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194711#sec010)

The Citation for the data:
```
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194711#sec010
```

### Exploring this dataset {-}
```{r}
wolves <- readxl::read_xlsx(
  here::here("data",
             "S1Table.xlsx")
) %>%
  as_tibble()

wolves
names(wolves)
```

### **Exercise 5 - Wolves**

A) Plot the relationship between the variables "Gross domestic product per capita (USD)" and "Population estimate". 

B) Transform both axes to a log10 scale

C) Add a linear relationship (`geom_smooth`, straight line)

D) Add a colour for each "Country group"

E) Now plot a linear relationship (straight line) for each "Country group"

```{r, include=FALSE}
wolves %>% 
  ggplot(aes(
    x = `Population estimate` %>% log10,
    y = `Gross domestic product per capita (USD)` %>% log10
  )) +
  geom_point(aes(colour = `Country group`)) +
  geom_smooth(
    aes(colour = `Country group`,
        group = `Country group`),
    method = "lm") 
```

F) Calculate the land area per "Country group" and plot this against the population estimate, what is your conclusion, considering the previous graph and this graph?
```{r}
wolves %>%
  group_by(`Country group`) %>%
  summarize(total_surface = sum(`Land area (km2)`),
            total_pop = sum(`Population estimate`)) %>%
  ggplot(aes(x = total_surface,
             y = total_pop)) +
  geom_col(aes(fill = `Country group`))


```

## Scientific Databases

There are many dedicated databases fro scientific data.

For example https://doi.org/10.3334/ORNLDAAC/1275
How to read netCDF files into R:
http://geog.uoregon.edu/bartlein/courses/geog490/week04-netCDF.html

### **EXERCISE 5 - Annual averages for temperature**

A) Load the above data from georg into R.
You need to manually download the netcdf file first
```{r}
library(ncdf4)
url <- "http://geog.uoregon.edu/GeogR/data/raster/cru10min30_tmp.nc"
download.file(url = url, destfile = "test.nc", mode = "wb")
ncin <- nc_open("test.nc")
ncin

```

B) Follow the tutorial at: http://geog.uoregon.edu/bartlein/courses/geog490/week04-netCDF.html. Read until (not including) paragraph: "4 Data frame-to-array conversion(rectangular to raster)"

C) Get the annual mean per monthfor all longitudes and latitudes, for the complete dataset, no missing values
```{r, include=FALSE}
lon <- ncvar_get(ncin,"lon")
nlon <- dim(lon)
head(lon)
lat <- ncvar_get(ncin,"lat")
nlat <- dim(lat)
head(lat)
# create dataframe -- reshape data
# matrix (nlon*nlat rows by 2 cols) of lons and lats
lonlat <- as.matrix(expand.grid(lon,lat))
dim(lonlat)
lonlat

dname <- "tmp"
# vector of `tmp` values
# get a single slice or layer (January)
tmp_array <- ncvar_get(ncin, "tmp")
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(tmp_array)
m <- 1
tmp_slice <- tmp_array[,,m]
# levelplot of the slice
library(lattice)
library(RColorBrewer)
grid <- expand.grid(lon=lon, lat=lat)

cutpts <- c(-50,-40,-30,-20,-10,0,10,20,30,40,50)
levelplot(tmp_slice ~ lon * lat, data=grid, at=cutpts, cuts=11, pretty=T, 
  col.regions=(rev(brewer.pal(10,"RdBu"))))


tmp_vec <- as.vector(tmp_slice)
length(tmp_vec)
tmp_df01 <- data.frame(cbind(lonlat, tmp_vec))
names(tmp_df01) <- c("lon","lat",paste(dname,as.character(m), sep="_"))
head(na.omit(tmp_df01), 10)

tmp_vec_long <- as.vector(tmp_array)
length(tmp_vec_long)

# get time
time <- ncvar_get(ncin,"time")
time
tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
# reshape the vector into a matrix
tmp_mat <- matrix(tmp_vec_long, nrow=nlon*nlat, ncol=nt)
dim(tmp_mat)

# create a dataframe
lonlat <- as.matrix(expand.grid(lon,lat))
tmp_df02 <- data.frame(cbind(lonlat,tmp_mat))
names(tmp_df02) <- c("lon","lat","tmpJan","tmpFeb","tmpMar","tmpApr","tmpMay","tmpJun",
  "tmpJul","tmpAug","tmpSep","tmpOct","tmpNov","tmpDec")
# options(width=96)
head(na.omit(tmp_df02, 20))
# get the annual mean and MTWA and MTCO
tmp_df02$mtwa <- apply(tmp_df02[3:14],1,max) # mtwa
tmp_df02$mtco <- apply(tmp_df02[3:14],1,min) # mtco
tmp_df02$mat <- apply(tmp_df02[3:14],1,mean) # annual (i.e. row) means
head(na.omit(tmp_df02))
tmp_df03 <- na.omit(tmp_df02)
head(tmp_df03)

```

D) What is the temperature pattern (make a graph) for lon = "4.75", lat =	"52.25"
You will need to use 'gather()' to reshape the data first to a stacked format
```{r, include=FALSE}
tmp_df03 %>%
  dplyr::filter(lon == "4.75", lat == "52.25") %>%
  gather(tmpJan:tmpDec, key = "month", value = "temp") %>%
  ggplot(aes(x = month, y = temp)) +
  geom_point() + toolboxr::rotate_axis_labels(axis = "x", angle = 45)


```

E) Review the chapter ["Factors"](https://r4ds.had.co.nz/factors.html) of "R for Data Science" and reorder the factor levels so that data per month follows the right order. Create a new graph, displaying the right order of months and corresponding temperature

**Tips:**
 
 - Maybe it is a good idea to strip the 'tmp' part from the month variable names
 - Use dplyr to create a new ordered factor for month.
 - create a vector with the proper month levels as your first step (these will be the new intended levels for the month variable)
 - use the dataframe `tmp_df03` from the step at for this question.
 - maybe use `forcats::fct_relevel()` in conjunction with `dplyr::mutate()`?
 
```{r, include=FALSE}
names(tmp_df03)
tmp_df03_new <- tmp_df03 %>%
  dplyr::filter(lon == "4.75", lat == "52.25") %>%
  gather(tmpJan:tmpDec, key = "month", value = "temp") %>%
  mutate(
    month = str_replace_all(
      string = month, 
      pattern = regex("tmp"),
      replacement = ""
  )) %>%
  mutate(
    month = forcats::fct_relevel(
      month, "Jan", "Feb", "Mar", "Apr", "May", "Jun", 
             "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"
         ))

tmp_df03_new %>%  
ggplot(aes(x = month, y = temp)) +
  geom_point() + 
  toolboxr::rotate_axis_labels(axis = "x", angle = 45) 

levels(tmp_df03_new$month)

```

F) Where on the planet was this measured (tip: Google is you friend) - find the address. Remember, this is an annual average ...

## Opening a .json format
```{r}
## http://www.programmingr.com/examples/reading-json-data/
library(jsonlite)
# r web / r json - get json data from url
json_file <- "https://ckan.dataplatform.nl/dataset/fcd7fb40-e11d-4200-8d69-6aef6dbd8e0a/resource/79298c25-76fe-4e2a-b2ce-193944abdc89/download/panden-json.zip"

download.file(json_file, destfile = "json.zip")
unzip("json.zip")
data <- fromJSON("panden-json.json")

df <- data$features %>% as_tibble
df
```

## Data Science Contests (Kaggle)

Kaggle is a great resource for learning Data Science

### **Exercise 6** 

A) Create a Kaggle account

B) Go over the analysis at https://www.kaggle.com/kailex/education-languages-and-salary 

C) What is the most preferred programming language on Kaggle 

D) What is the most preferred platform? 

## Amazon WebServices - S3 buckets

Amazon has many open datasets. The no SQL S3 system of Amazon and an API are easy to use for getting this data. Some datasets are quite bog, so getting them could take a while (around 5 minutes). A Spark backend is used to speed things up.

```{r, eval=FALSE}
## get data from 
## Bike Share 
# https://s3.amazonaws.com/capitalbikeshare-data/index.html
#Loading the rvest package
library('rvest')
#Specifying the url for desired website to be scraped
# amazon open data buckets
# https://registry.opendata.aws/
library("aws.s3")
library(sparklyr)
library(ropenaq)

y <- get_bucket(bucket = "nyc-tlc")
y

y[[2]]
#x <- get_object(y[[3]])
#x
key <- y[[3]]$Key
# ?s3read_using
nyc_tlc <- aws.s3::s3read_using(read.csv, object = key, bucket = "nyc-tlc")
nyc_tlc <- as_tibble(nyc_tlc)
nyc_tlc


## air quality data
##https://github.com/ropensci/ropenaq
## aws dataset (complete bucket) https://registry.opendata.aws/openaq/
## https://openaq.org/
## part of the data:
# devtools::install_github("ropensci/ropenaq")

i = 2

bucket_item <- y[[i]]

bucket_item$Bucket
bucket_item$Key

library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "local")

usercsv_tbl <- 
  sparklyr::spark_read_csv(sc,name = "usercsvtlb",
                              path = 
                   file.path("s3a",
                             bucket_item$Bucket,
                             bucket_item$Key),
                 memory = FALSE)

```


## Dedicated API's

## Scraping of websites

Climbing accidents
```{r}
library(rvest)
library(tidyverse)

url_accidents <- read_html("https://www.klimongevallen.nl/ongevallen/")

accidents_page <- url_accidents %>%
  html_nodes("#ongevallentable") 

table <- accidents_page[[1]] %>%
  html_table() %>%
as_tibble()

table
names(table)

ind <- str_detect(string = table$Letsel, 
           pattern = regex("overleden", ignore_case = TRUE))

deaths <- table[ind, ]

deaths 


ind <- str_detect(string = table$Letsel, 
                  pattern = regex("zwaar gewond", ignore_case = TRUE))


severe <- table[ind,]


deaths_severe <- dplyr::bind_rows(deaths, severe) 
deaths_severe %>%
  mutate(datum_new = lubridate::ymd(Datum)) %>%
  arrange(desc(datum_new)) %>%
  select(Datum, Activiteit, Locatie, Letsel) %>% knitr::kable()

injuries <- unique(table$Letsel) %>%
  enframe()


table_new <- table %>%
  mutate(Datum_new = lubridate::ymd(Datum)) %>%
  mutate(year = lubridate::floor_date(Datum_new, unit = "years")) %>%
  mutate(letsel_new = 
           str_replace_all(string = Letsel, pattern = regex("Geen.|Geen"),
                           replacement = NA_character_)) %>%
  na.omit()


stats <- table_new %>%
#  dplyr::filter() %>%
  group_by(Activiteit, year) %>%
  tally() %>%
  arrange(desc(n))
 

plot1 <- stats %>%
  ggplot(aes(x = year, y = n)) +
  geom_point(aes(colour = Activiteit), size = 3) +
  geom_smooth(aes(group = Activiteit,
                  colour = Activiteit), 
              method = "lm") +
#  citrulliner::theme_individual() +
  ggtitle("Gerapporteerd ongelukken per jaar")

ggsave(plot = plot1, filename = "years.svg",
       width = 18, height = 8, dpi = 300)
ggsave(plot = plot1, filename = "years.png",
       width = 18, height = 8, dpi = 300)

stats <- table %>%
  #  dplyr::filter() %>%
  group_by(Activiteit, Letsel) %>%
  tally() %>%
  arrange(desc(n))

stats

plot_def <- stats %>%
arrange(desc(n)) %>%
  na.omit() %>%
#  dplyr::filter(n > 1) %>%
  mutate(letsel_trunc = str_trunc(string = Letsel, width = 12)) %>%
  ggplot(aes(x = letsel_trunc, y = n)) +
  geom_point(aes(colour = Activiteit), position = "jitter") +
  coord_flip()

plot_def
```



## EXERCISES

1. ### CBS - Open Data
https://www.cbs.nl/nl-nl/onze-diensten/open-data/statline-als-open-data/snelstartgids

A. Follow the installation instrunctions for the R package `{cbsopendataR}`
```{r}
# install.packages("cbsodataR")
library(cbsodataR)
```

B. Print a table (to an Excel file, containing all avaiable datasets). Use a pipe, `{readxl}` and the function `cbs_get_toc()`
```{r}
toc <- cbs_get_toc()
head(toc)

toc$ShortTitle[1:10]

# Downloaden van gehele tabel (kan een halve minuut duren)
data <- cbs_get_data("83765NED")
head(data)

# Downloaden van metadata
metadata <- cbs_get_meta("83765NED")

typeof(metadata)
metadata$DataProperties
head(metadata$TableInfos %>% as_tibble())

```

C. Download a dataset via `{rdryad}`
https://github.com/ropensci/rdryad

Create 4 valuable visualizations for this data. You may freely choose your dataset and the visualizations as long as you formulate a relevant research (complete the full PPDAC cycle).

Good luck!




