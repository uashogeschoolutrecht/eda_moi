# Lab 7A; Text mining {#lab7atextmining}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Credits
This material is partly reproduced from a workshop during the R-Cafe at Utrecht University.

## Text Mining

> The process of deriving high-quality information from text. (Wikipedia, 2019)

Applications

- Text categorization
- Entity extraction
- Document summarization
- Sentiment analysis

## Text Mining with R 

Text mining in R is challenging (without external tools).

> We developed the tidytext R package because we were familiar with many methods for data wrangling and visualization, but couldn’t easily apply these same methods to text. (Silge and Robinson 2016)

- Text mining package `tidytext`
- Book "Text mining with R (Silge and Robinson, 2016)"
  - www.tidytextmining.com/

## Text Mining with R 

![](images/cover.png)

## Recap: Tidy data 

Tidy data has a specific structure (Wickham 2014):

- Each variable is a column
- Each observation is a row
- Each type of observational unit is a table

## Recap: Non-tidy data (iris)

\small
```{r echo=FALSE}
head(iris, 10)
```
## Recap: Tidy data (iris)

\small
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
iris_rn = iris %>% mutate(id = row_number())
gather(iris_rn, "measure", "value", -id, -Species) %>% 
  arrange(id, desc(measure)) %>% 
  select(id, Species, measure, value) %>% 
  head(10)
```

## Tidy text

> Definition: *tidy text format is a table with **one-token-per-row***

- A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. 

## Packages for text mining

\small
```{r install, echo=T, message=FALSE, warning=FALSE, results='hide'}

# default tidyverse packages
library(tidyverse)
library(lubridate)

# text mining related
library(tidytext)
library(textdata)
library(wordcloud)
```


## Tweets by Donald Trump - load data

- http://trumptwitterarchive.com/
- https://github.com/mkearney/trumptweets/
- https://github.com/UtrechtUniversity/R-data-cafe/

\small
```{r}

tweets_trump <- read_csv(
  "https://raw.githubusercontent.com/UtrechtUniversity/R-data-cafe/master/themes/TextMining/data/trumptweets-1515775693.csv"
)
```
## Tweets by Donald Trump - preview

\small
```{r explore1, echo = TRUE}

head(tweets_trump)
```

## Tweets by Donald Trump - timeline

\scriptsize
```{r fig.height=2.5, fig.width=5, message=FALSE, warning=FALSE}
tweets_trump %>% 
  ggplot(aes(created_at)) + 
  geom_histogram() 
```


## Tweets by Donald Trump - tokenizing & tidy text

\scriptsize
```{r}
unnest_tokens(tweets_trump, word, text)
```

## Tweets by Donald Trump - tokenizing & tidy text

\scriptsize
```{r}
(tweets_trump_tokens <- unnest_tokens(tweets_trump, word, text, token="tweets"))
```
## Tweets by Donald Trump - timeline 'fake'

\scriptsize
```{r fig.height=2.5, fig.width=5, message=FALSE, warning=FALSE}
tweets_trump_tokens %>% 
  filter(word == "fake") %>% 
  ggplot(aes(created_at)) + 
  geom_histogram() 
```

## Tweets by Donald Trump - Most common words

Use `count()`, a function from the `dplyr` package!

\scriptsize
```{r}
tweets_trump_tokens %>% 
  count(word, sort = TRUE) %>% 
  head(10)
```

## Tweets by Donald Trump - Filter stopwords

Use `anti_join()`, a function from the `dplyr` package!

\scriptsize
```{r stopwords}
data(stop_words)

tweets_trump_tokens %>% anti_join(stop_words, by="word")
```

## Tweets by Donald Trump - Filter stopwords

\scriptsize
```{r}
tweets_trump_tokens %>% 
  anti_join(stop_words, by="word") %>% 
  count(word, sort = TRUE) %>% 
  head(10)
```

## Tweets by Donald Trump - Filter stopwords

\small
```{r eval=FALSE}
# top 20 non-stop words
tweets_trump_tokens %>% 
  anti_join(stop_words, by="word") %>% 
  count(word, sort = TRUE) %>%
  head(20) %>%
  
  # trick to reorder factor (for plotting purposes)
  mutate(word = reorder(word, n)) %>%
  
  # create plot with ggplot
  ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()
```

## Tweets by Donald Trump - Filter stopwords

```{r echo=FALSE}
# top 20 non-stop words
tweets_trump_tokens %>% 
  anti_join(stop_words, by="word") %>% 
  count(word, sort = TRUE) %>%
  head(20) %>%
  
  # trick to reorder factor (for plotting purposes)
  mutate(word = reorder(word, n)) %>%
  
  # create plot with ggplot
  ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()
```


## Tweets by Donald Trump - Word cloud

\scriptsize
```{r fig.height=2.5, fig.width=5, eval=FALSE}
library(wordcloud)

tweets_trump_tokens %>% 
  anti_join(stop_words, by="word") %>% 
  count(word, sort = TRUE) %>%
  # apply wordcloud to our data 
  with(wordcloud(word, n, scale=c(2, 1), max.words = 50))
```
## Tweets by Donald Trump - Word cloud

\scriptsize
```{r echo=FALSE}
library(wordcloud)

tweets_trump_tokens %>% 
  anti_join(stop_words, by="word") %>% 
  count(word, sort = TRUE) %>%
  # apply wordcloud to our data 
  with(wordcloud(word, n, scale=c(2, 1), max.words = 50))
```

## Sentiment analysis

> Systematically identify, extract, quantify, and study affective states and subjective information. (Wikipedia, 2019)

## Sentiment libraries

- Avialable in package `textdata`.

\small
```{r}
get_sentiments("afinn")
```

## Sentiment libraries

- Not every English word is in the lexicons because many English words are pretty neutral. 

\small
```{r}
get_sentiments("bing")
```

## Sentiment libraries

- It is important to keep in mind that these methods do not take into account qualifiers before a word, such as in “no good” or “not true”

\small
```{r}
get_sentiments("nrc")
```

## Tweets by Donald Trump - Sentiment analysis [bing]

\scriptsize
```{r}
(tweet_sentiment <- tweets_trump_tokens %>%
   # append score/value to each word (if and only if available)
   left_join(get_sentiments("bing"), by="word") %>%
   count(created_at, status_id, sentiment) %>% 
   # untidy the dataset to compute the sentiment
   spread(sentiment, n, fill = 0) %>%
   # sentiment is number of positive words - negative words
   mutate(sentiment = positive - negative))
```

## Tweets by Donald Trump - Sentiment analysis [bing]

\scriptsize
```{r fig.height=2, fig.width=5}
tweet_sentiment %>% 
  group_by(month=floor_date(created_at, "month")) %>%
  summarize(sentiment=mean(sentiment)) %>% 
  ggplot(aes(month, sentiment)) + 
    geom_line()
```

## Tweets by Donald Trump - Sentiment analysis [afinn]

\scriptsize
```{r fig.height=2, fig.width=5}
tweets_trump_tokens %>%
  left_join(get_sentiments("afinn"), by="word") %>%
  mutate(value = replace_na(value, 0)) %>%
  group_by(month=floor_date(created_at, "month")) %>%
  summarize(sentiment=mean(value)) %>% 
  ggplot(aes(month, sentiment)) + 
    geom_line()
```

## Text Classification

> Text classification is the process of assigning text into organized groups.

Applications

- Customer service
- Language Detection
- Spam filters
- ...

## Text Mining with R 

Text mining in R is challenging (without external tools).

> We developed the tidytext R package because we were familiar with many methods for data wrangling and visualization, but couldn’t easily apply these same methods to text. (Silge and Robinson 2016)

- Text mining package `tidytext`
- Book "Text mining with R (Silge and Robinson, 2016)"
  - www.tidytextmining.com/

## Text Mining with R 

![](images/cover.png)

## Recap: Tidy data 

Tidy data has a specific structure (Wickham 2014):

- Each variable is a column
- Each observation is a row
- Each type of observational unit is a table

## Recap: Non-tidy data (iris)

\small
```{r echo=FALSE}
head(iris, 10)
```
## Recap: Tidy data (iris)

\small
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
iris_rn = iris %>% mutate(id = row_number())
gather(iris_rn, "measure", "value", -id, -Species) %>% 
  arrange(id, desc(measure)) %>% 
  select(id, Species, measure, value) %>% 
  head(10)
```

## Tidy text

> Definition: *tidy text format is a table with **one-token-per-row***

- A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. 

## Packages for text classification

\small
```{r install, echo=T, message=FALSE, warning=FALSE, results='hide'}

# default tidyverse packages
library(tidyverse)
library(lubridate)

# text mining related
library(tidytext)
library(tm)
library(textdata)
library(wordcloud)

# machine learning
library(caret)
library(randomForest)
```


## The President's Weekly Address

- Example: https://www.presidency.ucsb.edu/documents/the-presidents-weekly-address-431

\small

My fellow Americans, the heartbreaking devastation and suffering caused by Hurricane Harvey has profoundly affected our entire Nation. Many homes and communities have been destroyed, many lives have been upended, and tragically, some have lost their lives in this catastrophic storm. We pray for the victims and their families and all of those who have been displaced from their homes.

At this very moment, heroic efforts continue to keep safe those threatened by this natural disaster. I want to say a special word of thanks to our amazing first responders: our police and law enforcement officers, firefighters, Coast Guard, National Guard, EMS, doctors, nurses, hospital workers, and volunteers who have traveled from all across the country. Thousands of people have come together to prevent loss of life and ensure safety, and we are incredibly grateful for their courage, their professionalism, and their sacrifice. They are an inspiration to all of us.


## The President's Weekly Address - read data

- Run `data_downloader.R`

\small
```{r}

president_speeches <- read_csv(
  "data/speeches.csv", 
  col_types = cols(name="f")
) %>% 
  mutate(date=mdy(date))
```

## The President's Weekly Address - preview

\scriptsize
```{r explore1, echo = TRUE}
head(president_speeches)
```

## The President's Weekly Address

\small
```{r fig.height=2, fig.width=3, message=FALSE, warning=FALSE}
president_speeches %>% 
  ggplot() + 
  geom_histogram(aes(name), stat="count")
```



## The President's Weekly Address - tokenizing & tidy text

\scriptsize
```{r}
(president_tokens <- unnest_tokens(president_speeches, word, speech))
```


## The President's Weekly Address - Word cloud

\scriptsize
```{r eval=FALSE, message=FALSE, warning=FALSE}
library(wordcloud)

president_tokens %>% 
  filter(name=="Donald J. Trump") %>%
  anti_join(stop_words, by="word") %>% 
  count(word, sort = TRUE) %>%
  with(
    wordcloud(
      word, n, scale=c(5, 1), 
      max.words = 100, 
      random.order=FALSE, 
      rot.per=0.3,
      colors='blue')
  )

```

## The President's Weekly Address - Word cloud Trump

\scriptsize
```{r echo=FALSE, fig.height=12, fig.width=12, message=FALSE, warning=FALSE}
library(wordcloud)

president_tokens %>% 
  filter(name=="Donald J. Trump") %>%
  anti_join(stop_words, by="word") %>% 
  count(word, sort = TRUE) %>%
  with(
    wordcloud(
      word, n, scale=c(5, 1), 
      max.words = 100, 
      random.order=FALSE, 
      rot.per=0.3,
      colors='blue')
  )

```

## The President's Weekly Address - Word cloud Obama

\scriptsize
```{r echo=FALSE, fig.height=12, fig.width=12, message=FALSE, warning=FALSE}
library(wordcloud)

president_tokens %>% 
  filter(name=="Barack Obama") %>%
  anti_join(stop_words, by="word") %>% 
  count(word, sort = TRUE) %>%
  with(
    wordcloud(
      word, n, scale=c(5, 1), 
      max.words = 100, 
      random.order=FALSE, 
      rot.per=0.3,
      colors='red')
  )

```


## Term frequency

- **Term Frequency (tf)** - A measure of how important a word is.
- *How frequently a word occurs in a document. There are words in a document, however, that occur many times but may not be important; in English, these are probably words like “the”, “is”, “of”, and so forth. We might take the approach of adding words like these to a list of stop words and removing them before analysis, but it is possible that some of these words might be more important in some documents than others. (Text Mining with R, 2019)*

## The President's Weekly Address - Term frequency

\scriptsize
```{r}
president_words <- president_tokens %>% count(id, name, word, sort=TRUE)

total_words <- president_words %>% 
  group_by(id) %>% 
  summarize(total = sum(n))

(president_words <- left_join(president_words, total_words, by = "id"))
```

## Term frequency - Zipf's distribution

- Long-tailed distribution (Zipf's)

\scriptsize
```{r fig.height=2, fig.width=3, message=FALSE, warning=FALSE}
ggplot(president_words, aes(n/total)) +
  geom_histogram(bins=50)
```

<!-- In fact, those types of long-tailed distributions are so common in any given corpus of natural language (like a book, or a lot of text from a website, or spoken words) that the relationship between the frequency that a word is used and its rank has been the subject of study; a classic version of this relationship is called Zipf’s law, after George Zipf, a 20th century American linguist. -->

## Inverse Document Frequency

- **Inverse Document Frequency (IDF)** - IDF decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. 
- *The statistic **tf-idf** is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites. (Text Mining with R, 2019)*

## The President's Weekly Address - Inverse Document Frequency

\scriptsize
```{r}
(president_words <- president_words %>%
  bind_tf_idf(word, id, n))
```


## The President's Weekly Address - TF-IDF

\scriptsize
```{r fig.height=2, fig.width=3, message=FALSE, warning=FALSE}
president_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  top_n(15) %>% 
  ggplot(aes(word, tf_idf)) +
    geom_col() +
    labs(x = NULL, y = "tf-idf") +
    coord_flip()
```


## from tidy to DocumentTermMatrix

Tidy text format is not suitable for machine learning.

\scriptsize
```{r}
library(tm)

(president_tfidf = president_words %>%
  
  arrange(id) %>%
  
  # cast to document term matrix
  cast_dtm(id, word, tf_idf) %>%
  
  # remove sparse terms
  removeSparseTerms(0.9))

```


## Split dataset into train and test set

\scriptsize
```{r}
library(caret)

set.seed(535)

trainIndex <- createDataPartition(president_speeches$name, p = .6, 
                                  list = FALSE, 
                                  times = 1)

presidentTrain <- president_tfidf[ trainIndex,]
presidentTest  <- president_tfidf[-trainIndex,]

presidentTrain
```

## Machine Learning - Random Forest

- Fit Random Forest on the train data

\scriptsize
```{r message=FALSE, warning=FALSE}

library(randomForest)

(classifier <- randomForest(
  x = as.data.frame(as.matrix(president_tfidf[trainIndex,])), 
  y = as.factor(president_speeches[trainIndex,]$name),
  nTree = 10))
```

## Machine Learning - Random Forest

- Fit Random Forest on the train data

\scriptsize
```{r}
(president_pred <- predict(
  classifier, 
  newdata = as.data.frame(as.matrix(president_tfidf[-trainIndex,]))
))
```


## Validation - Confusion matrix

- https://en.wikipedia.org/wiki/Confusion_matrix

\small
```{r}
(cm <- table(
  president_speeches[-trainIndex,]$name, 
  president_pred
))
```

