# Lab 6A; Exploring Assumptions & Models {#lab6assumptionsmodels}

## Contents of this chapter

 - Testing Assumptions
 - Data transformations
 - Exploring clusters (Principal Component Analysis)
 - Using regression to discover patterns (`gapminder data`)
 - Logistic regression example (`AppliedPredictiveModelling`)
 - Random Forest (as an example of Tree-based regression) (titanic data)
 - How to handle many models in R
 - Learning more: tutorials from the `{adventr}` and the `{parsnip}` package
 
## Introduction
This chapter is not a chapter on statistics. It can however be a primer to learn more about using R for performing more formal statitical inference. With this chapter, I hope to inspire the reader to try and use models (statistical models if you will) to do Exploratory Data Anlysis. Although EDA is often said to not depend on statistics, I do not agree. Statistical inference and predictive modelling can learn you a lot about you data and provide valuable inside on where to go next in you analysis. 
There are many good works on statitics and R. Three books stand out and provided valuable information for this chapter:
[@dsur], [@apm], [@rethinking]. For a complete overview and solid work on using R for inference: [@adventr] and [@dsur]

## Packages
```{r}
library(tidyverse)
library(AppliedPredictiveModeling)
library(tidyverse)
library(devtools)
library(pastecs)
library(car)
library(e1071)
library(pastecs)
library(caret)
library(AppliedPredictiveModeling)
# install_github("profandyfield/adventr")
library(adventr)
# devtools::install_github("tidymodels/parsnip")
library(parsnip)
library(tidymodels)
library(recipes)
```

## Assumptions
Every model has assumptions that are relevant for the applicability of a model. We must assume that a model is exactly what it is: a model and as such it is a representation of something else. In Data Science we use models to discribe or even predict thinkgs about the world surrounding us. 

In statistical inference and also for EDA assumptions are important. They are the prerequisites for the applicability for the models we are using.
Assumptions for statical inference and models can be requirements for a distribution type, variable type, number of groups to compare, equality of variance. Usually we consider a statistical test invalid if it is performed on data that does not meet one more assumptions. A robust test is relatively uninfluenced by one or more assumptions. In this chapter, I will not extensively address statitistical inference and robustness. We will however learn how to assess whether some common assumptions like distribution requirements and eqaulity of variance between groups are met. After which we will explore how to use models to do EDA.    

### Testing for a normal distribution
We can 'formally assess whether a distribution is normal.
Below is a complete worked out example on how to test for the assumption of normality and equality of variance. The example is taken from [@dsur].

### Read in the 'Festival Dataset' from [@dsur]:
```{r}
dlf <- read_delim(file = here::here("data", 
                                   "DownloadFestival.dat"), 
                                   delim =  "\t", na = c("", " "))
dlf %>% head(3)
```

Checking missing values, distributions and detecting outliers
```{r}
sum(is.na(dlf))
x <- summary(dlf)
min_maxs <- x[c(1, 6), c(3:5)] %>% unlist() %>% print()
naniar::vis_miss(dlf)
```

Detecting an outlier with a histogram
```{r}
hist.outlier <- ggplot(dlf, aes(day1)) + 
  geom_histogram(aes(y=..density..), 
                 colour="black", 
                 fill="white") + 
  labs(x="Hygiene score on day 1", y = "Density") +
  theme(legend.position = "none")
hist.outlier
```

`{ggplot2}` works best with long or so-called stacked datasets.
```{r}
dlf_long <- dlf %>% 
  tidyr::gather(day1:day3, key = "days", value = "hygiene_score")
dlf_long
```

Boxplots with outlier
```{r, echo=FALSE}
hist.boxplot <- dlf_long %>%
  ggplot(aes(x = days, y = hygiene_score)) + 
  geom_boxplot(aes(group = days)) +
  geom_point(data = dplyr::filter(dlf_long, hygiene_score > 19), 
             colour = "darkred", size = 2.5) +
  labs(x="Hygiene score on day 1", y = "Hygiene Score") +
  theme(legend.position = "none") + 
  facet_wrap(~ gender)
hist.boxplot
```

Remove outlier
```{r}
dlf <- dlf %>%
  dplyr::filter(!day1 > 19)

dlf_long <- dlf_long %>%
  dplyr::filter(!hygiene_score > 19)

```

Boxplots without outlier
```{r}
hist.boxplot <- dlf %>%
  tidyr::gather(day1:day3, key = "days", value = "hygiene_score") %>%
  ggplot(aes(x = days, y = hygiene_score)) + 
  geom_boxplot(aes(group = days)) + 
  labs(x="Hygiene score on day 1", y = "Hygiene Score") +
  theme(legend.position = "none") + 
  facet_wrap(~ gender)
hist.boxplot
```

### All data
At this point it is wise to look at all the data
```{r}
dlf_long %>%
  ggplot(aes(x = hygiene_score, y = ticknumb)) +
  geom_point(aes(colour = days)) +
  facet_wrap(~gender)
```

### Distributions for hygiene scores on day 1, day 2 and day 3. Here we disregard the gender variable, assuming there is no difference in hygiene score between males and females (which could be a dangerous assumption). We will come back to this later.
```{r}
dlf_long %>%
  ggplot(aes(x = hygiene_score)) +
  geom_density(aes(colour = days)) +
  facet_wrap(~days)
```
Are these distributions following a Gaussian bell-shaped curve?

### How would the distribution look if it were Gaussian?
We add the simulated normal distributions to the original dataframe and create a new stacked version. We set the seed for reproducibility.
```{r}
set.seed(123)
## add normal distribution to the data (based on observed mean and sd per day)
dlf_norm <- dlf %>%
  mutate(
    norm_day_1 = rnorm(
      mean = mean(dlf$day1, na.rm = TRUE), 
      n = nrow(dlf), 
      sd = sd(dlf$day1, na.rm = TRUE)),
    norm_day_2 = rnorm(
      mean = mean(dlf$day2, na.rm = TRUE), 
      n = nrow(dlf), 
      sd = sd(dlf$day2, na.rm = TRUE)),
    norm_day_3 = rnorm(
      mean = mean(dlf$day3, na.rm = TRUE), 
      n = nrow(dlf), 
      sd = sd(dlf$day3, na.rm = TRUE))) %>%
  dplyr::select(gender, norm_day_1:norm_day_3) %>%
  tidyr::gather(norm_day_1:norm_day_3, 
                key = "days", 
                value = "norm_hygiene_score")
  

## add to plot
dlf_long %>%
  dplyr::filter(!hygiene_score > 19) %>%
  ggplot(aes(x = hygiene_score)) +
  geom_density(aes(colour = days)) +
  geom_density(data = dlf_norm, aes(x = norm_hygiene_score,
                                    colour = days)) +
  facet_wrap(~days)
  
```

### qq-plot
The quantile-quantile ploit shows the realtionship between the true data distribution and the estimated distribution, under assumption of normality

Q-Q plot for day 1
```{r}
## see the file ggqq.R for the function definition
source(file = here::here("code", "ggqq.R"))
gg_qq_1 <- gg_qq(dlf$day1)
gg_qq_1
```

Q-Q Day 2
```{r}
gg_qq_2 <- gg_qq(dlf$day2)
gg_qq_2
```
Clearly not normally distributed

Q-Q Day 3
```{r}
gg_qq(dlf$day3)
```
Not evenly distributed and not a good fit to the estimated distribution.

## Skewness and kurtosis
Skewness and kurtosis are parameters that display the deviation from normality looking at the shape of the distribution polynom. A distribution with an absolute `skew.2SE` > 1 is significantly skewed and not normal. A distribution with an absolute `kurt.2SE` > 1 has significant kurtosis and is not normally distributed. 

`kurt.2SE` and `skew.2SE` are calculated from

 - $kurt.2SE = kurt / 2*(standard.error)$ 
 - $skew.2SE = skew / 2*(standard.error)$
 
## `Shapiro-Wilk test` 
To test for normality we can use the `Shapiro-Wilk test`. This test checks whether the deviation from normality is significant (H0) or not (H1), 

 - p-value < 0.05 means that the distribution is significantly different from a normal distribution: assumption "the distribution is not normal"
 - p-value > 0.05 means that the distribution is not significantly different from normal: assumption "the distribution cannot be proved to deviate from normal"
 
## `stat.desc()` to get descriptive statistics for a dataframe or a variable 
```{r}
round(stat.desc(dlf[, c("day1", "day2", "day3")], basic = FALSE, norm = TRUE), digits = 3)
```

## Levene's Test
The Levene Test can be used to asses whether the variance for two or more distributions is equal. As always, when using the Levene Test it is important to assess also visually if the outcome of the statitical test makes sense. 

```{r, eval=FALSE}
leveneTest(dlf$day1, dlf$day2)
leveneTest(data = dlf_long, hygiene_score ~ days)
leveneTest(data = dlf_long, hygiene_score ~ days * gender)

dlf_long %>%
  ggplot(aes(x = hygiene_score, y = ticknumb)) +
  geom_point(aes(colour = days)) +
  facet_wrap(days~gender, nrow = 3)
```
A significant Levene Test indicates that the H0 = variances are equal does not hold and can be rejected. So if we look at the days as a factor, variance is equal over the days, but if we also take gender into consideration, we see that the variances between all groups are not equal.

## Transforming data

 - To remove skewness or kurtosis
 - Apply the __*same*__ transformation to __*all variables*__
 - After transformation and analysis or especially with predictions, sometimes you need to inverse-transform to make sense of the outcome. 
 - It can be a time comsuming process: 'trial-and-error'

### Log, square root and inverse
Sometimes skewness can be greatly reduced by applying a log (10, n or 2) transformation to the data. We already saw an example of this in the R-exam data above.

### Center and scale
Centering and scaling is the most simple transfromation. 
When centering a variable, the average value is subtracted from all the values, resulting in a zero mean. To scale the data, each value of the variable is divided by its standard deviation.

A modern way in R to do preprocessing and modelling of data is through the combination with:

 - `{recipes}`
 - `{parsnip}`
 - `{tidymodels}`

These three packages are build to work with all the `{tidyverse}` tools you have learned about so far. One of the developers of these packages is Max Kuhn, the author of the famous predictive modelling package `{caret}`.

The code below was taken from: https://github.com/tidymodels/parsnip 
https://tidymodels.github.io/parsnip/articles/parsnip_Intro.html


### Recipes

A _recipe_ can be trained then applied to any data. 

```{r rec_basic}
library(recipes) 
library(dplyr)
library(caret)
data("Sacramento")

## Create an initial recipe with only predictors and outcome
rec <- recipe(price ~ type + sqft, data = Sacramento)
rec <- rec %>% 
  step_log(price) %>%
  step_dummy(type, one_hot = TRUE)
rec_trained <- prep(rec, training = Sacramento, retain = TRUE)
design_mat <- bake(rec_trained, new_data = Sacramento)
design_mat
```

**EXERCISE**

 1. Create a plot using the `Sacramento` dataframe, showing the relationship between the variables `type`, `sqft` and `price`. Consider price to be the dependent variable that should go on the y-axis. The rest of the plot is up to you.
 1. Create the same graph as above but now log10-transform the `price` variable
 1. Now add facets to accomodate the variables `baths` and `beds`
 1. Create a summary tables that displays the amount of observations for eacht level in the factorvariable `type`
 1. What can you conclude from this exploration? Write a short statement.


```{r, include=FALSE}

## 1.
Sacramento %>%
  ggplot(aes(x = sqft,
             y = price)) +
  geom_point(aes(colour = type), alpha = 0.3)  +
  guides(colour = guide_legend(override.aes = list(alpha = 1)))

## 2.
Sacramento %>%
  ggplot(aes(x = sqft,
             y = log10(price))) +
  geom_point(aes(colour = type), alpha = 0.3)

## 3.
Sacramento %>%
  ggplot(aes(x = sqft,
             y = log10(price))) +
  geom_point(aes(colour = type), alpha = 0.3) +
  facet_grid(beds ~ baths)

## 4.
## summary
Sacramento %>%
  dplyr::group_by(type) %>%
  tally()


```

### Include the log transofrmation step in the recipe
```{r}
rec <- recipe(price ~ type + sqft, data = Sacramento)
rec <- rec %>% 
  step_log(price) %>%
  step_dummy(type, one_hot = TRUE) %>%
  step_log(price)
 
rec_trained <- prep(rec, training = Sacramento, retain = TRUE)
design_mat <- bake(rec_trained, new_data = Sacramento)
design_mat

```

## We can explore this data with Principal Component Analysis
The steps to perform preprocessing, fitting and prediction are:

 1. Build a recipe
 1. Use `prep()` to prepare the recipe
 1. `bake()` the preparation
 1. Use `predict()` to predict 'new' or 'test' data

```{r}
rec <- recipe(~ ., data = Sacramento)
pca_trans <- rec %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_pca(all_numeric(), num_comp = 3)
pca_estimates <- prep(pca_trans, training = Sacramento)

##
pca_data <- bake(pca_estimates, Sacramento)


pca_data %>%
  ggplot(aes(x = PC1,
             y = PC2)) +
  geom_point(aes(colour = Sacramento$type), alpha = 0.4)

pca_data %>%
  ggplot(aes(x = PC1,
             y = PC3)) +
  geom_point(aes(colour = Sacramento$type), alpha = 0.4)

pca_data %>%
  ggplot(aes(x = PC2,
             y = PC3)) +
  geom_point(aes(colour = Sacramento$type), alpha = 0.4)

```

### Creating models with `{parsnip}`

The idea of parsnip is to:

 - Separate the definition of a model from its evaluation.
 - Decouple the model specification from the implementation (whether the implementation is in R, spark, or something else). For example, the user would call rand_forest instead of ranger::ranger or other specific packages.
 - Harmonize the argument names (e.g. n.trees, ntrees, trees) so that users can remember a single name. This will help across model types too so that trees will be the same argument across random forest as well as boosting or bagging.

### Let's look at linear regression first
Here we use mtcars for simplicity
```{r}
# library(parsnip)
names(mtcars)

mtcars %>%
  ggplot(aes(x = disp,
             y = mpg)) +
  geom_point()


norm_recipe <- 
  recipe(
    mpg ~ disp, 
    data = mtcars) %>% 
  # estimate the means and standard deviations
  prep(training = mtcars, retain = TRUE)


lin_reg <- linear_reg(mode = "regression") %>%
  set_engine("lm") %>%
  fit(mpg ~ disp, data = mtcars) %>%
  broom::tidy()

lin_reg   

## plot the line in the scatterplot
mtcars %>%
  ggplot(aes(x = disp,
             y = mpg)) +
  geom_point() +
  geom_abline(intercept = lin_reg$estimate[1],
              slope = lin_reg$estimate[2])

```

### Extending the regression with more predictors
```{r}
norm_recipe_multiple <- 
  recipe(
    mpg ~ ., 
    data = mtcars) %>% 
  # estimate the means and standard deviations
  prep(training = mtcars, retain = TRUE)

lin_reg_multiple <- linear_reg(mode = "regression") %>%
  set_engine("lm") %>%
  fit(mpg ~ ., data = mtcars) 

lin_reg_multiple %>%
  broom::tidy()


## plot the line in the scatterplot
mtcars %>%
  ggplot(aes(x = wt,
             y = mpg)) +
  geom_point() +
  geom_abline()

mtcars %>%
  ggplot(aes(x = drat,
             y = mpg)) +
  geom_point() +
  geom_abline()


```

### Predict mpg from a number of observations
We create a split of the original data (20% test set, 80% training)
```{r}
cars_split <- initial_split(mtcars)
cars_train <- training(cars_split)
cars_test <- testing(cars_split)

## to prevent 'leakage of test data into the training set, we apply the recipe to the training set only
norm_recipe_train <- 
  recipe(
    mpg ~ ., 
    data = cars_train) %>% 
  # estimate the means and standard deviations
  prep(training = cars_train, retain = TRUE)

lin_reg_train <- linear_reg(mode = "regression") %>%
  set_engine("lm") %>%
  fit(mpg ~ ., data = juice(norm_recipe_train)) 


preds <- predict(object = lin_reg_train, new_data = cars_test)

## check performance
validate <- dplyr::bind_cols(mpg_original = cars_test$mpg, 
                             predictions = preds$.pred)

validate %>%
  ggplot(aes(x = mpg_original,
             predictions)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0)
```

### Can we improve with another fit?
```{r}

rand_forest_fit <- rand_forest(mode = "regression") %>%
  set_engine("ranger") %>%
  fit(mpg ~ ., data = juice(norm_recipe_train)) 

rand_forest

preds <- predict(object = rand_forest_fit, new_data = cars_test)

## check performance
validate <- dplyr::bind_cols(mpg_original = cars_test$mpg, 
                             predictions = preds$.pred)

validate %>%
  ggplot(aes(x = mpg_original,
             predictions)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0)


```

### General linear model vs Random Forest to make predictions and learn about the data
Here we use the Ames Housing data 

```{r}
library(AmesHousing)
ames <- make_ames()

library(tidymodels)

set.seed(4595)
## use stratification
data_split <- initial_split(ames, strata = "Sale_Price", p = 0.75)

ames_train <- training(data_split)
ames_test  <- testing(data_split)

```

### Random Forests
```{r}
rf_defaults <- rand_forest(mode = "regression")
rf_defaults

## using the formula approach
rand_forest(mode = "regression", mtry = 3, trees = 1000) %>%
  set_engine("ranger") %>%
  fit(
    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,
    data = ames_train
  )
```

### Penalized Logistic Regression

A linear model might work here too. The linear_reg model can be used. To use regularization/penalization, there are two engines that can do that here: the glmnet and sparklyr packages. The former will be used here and it only implements the non-formula method. parsnip will allow either to be used though.

When regularization is used, the predictors should first be centered and scaled before given to the model. The formula method won’t do that so some other methods will be required. We’ll use recipes package for that (more information here).

```{r}

norm_recipe <- 
  recipe(
    Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold, 
    data = ames_train
  ) %>%
  step_other(Neighborhood) %>% 
  step_dummy(all_nominal()) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_log(Sale_Price, base = 10) %>% 
  # estimate the means and standard deviations
  prep(training = ames_train, retain = TRUE)

# Now let's fit the model using the processed version of the data

glmn_fit <- 
  linear_reg(penalty = 0.001, mixture = 0.5) %>% 
  set_engine("glmnet") %>%
  fit(Sale_Price ~ ., data = juice(norm_recipe))


glmn_fit


test_normalized <- bake(norm_recipe, new_data = ames_test, all_predictors())

pred <- predict(glmn_fit, new_data = test_normalized) 

results <- dplyr::bind_cols(ames_test, pred)
names(results)
results %>%
  ggplot(aes(x = Sale_Price, y = .pred)) +
  geom_point()

```

### Let's look at the effect of centering and scaling with a Principal Component Analysis:
```{r}

iris_pca <- function(center, scale){

data(iris)
iris <- tibble::as_tibble(iris)
log_iris <- log(iris[, 1:4]) ## only numeric variables can be used for PCA
iris_species <- iris[, 5] ## labels

iris_pca <- stats::prcomp(log_iris,
                   center = center,
                   scale = scale) 

plot <- as_tibble(iris_pca$x) %>%
  mutate(description = iris_species$Species) %>%

  ggplot(aes(x = PC1, y = PC2, color = description)) +
  geom_point() +
  theme_bw() 

return(plot)

}

iris_pca(scale = FALSE, center = FALSE)
iris_pca(scale = TRUE, center = TRUE)

```

## Using Box-Cox transformations
In order to spead up the process of finding the right transformation we can preform a Box-Cox transformation which will yield us a lambda value. Below I who an example. From [@apm]
The estimated paramter in the Box-Cox transformation is Lambda.

Below are examples of how λ is implemented for a transformation on predictor variablel Y

Lambda value (λ) | Transformed data (Y’)
---------------------------------------
-3 	             | \[Y^{-3} = 1/Y^{3}\]
-2 	             | \[Y^{-2} = 1/Y^{2}\]
-1 	             | \[Y^{-1} = 1/Y^{1}\]
-0.5 	           | \[Y^{-0.5} = 1/(√Y)\]
etc.


```{r}
data(segmentationOriginal)
segmentationOriginal %>% as_tibble()

## Retain the original training set
segTrain <- subset(segmentationOriginal, Case == "Train")

## Remove the first three columns (identifier columns)
segTrainX <- segTrain[, -(1:3)]
segTrainClass <- segTrain$Class

## difference between max an minimal value (cutoff 20x)
max(segTrainX$VarIntenCh3)/min(segTrainX$VarIntenCh3)
skewness(segTrainX$VarIntenCh3)

## Use caret's preProcess function to transform for skewness
segPP <- preProcess(segTrainX, method = "BoxCox")

## Apply the transformations
segTrainTrans <- predict(segPP, segTrainX)

## Results for a single predictor
segPP$bc$VarIntenCh3

histogram(~segTrainX$VarIntenCh3,
          xlab = "Natural Units",
          type = "count")

histogram(~log(segTrainX$VarIntenCh3),
          xlab = "Log Units",
          ylab = " ",
          type = "count")
```

## Logistic Regression
```{r, eval=FALSE}
learnr::run_tutorial("adventr_log", package = "adventr")
```

